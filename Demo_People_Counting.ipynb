{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Demo People Counting",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02ac0588602847eea00a0205f87bcce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_091fdf499bd44a80af7281d16da4aa93",
              "IPY_MODEL_c79f69c959de4427ba102a87a9f46d80"
            ],
            "layout": "IPY_MODEL_c472ea49806447a68b5a9221a4ddae85"
          }
        },
        "091fdf499bd44a80af7281d16da4aa93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a90f72d3a2d46cb9ad915daa3ead8b4",
            "max": 819257867,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c42ae5af74a0491187827d0a1fc259bb",
            "value": 819257867
          }
        },
        "c79f69c959de4427ba102a87a9f46d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fead0160658445bf9e966daa4481cad0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2a7ed6611da34662b10e37fd4f4e4438",
            "value": " 781M/781M [00:23&lt;00:00, 35.1MB/s]"
          }
        },
        "c472ea49806447a68b5a9221a4ddae85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a90f72d3a2d46cb9ad915daa3ead8b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c42ae5af74a0491187827d0a1fc259bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "fead0160658445bf9e966daa4481cad0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a7ed6611da34662b10e37fd4f4e4438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf1ab9fde7444d3e874fcd407ba8f0f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_933ebc451c09490aadf71afbbb3dff2a",
              "IPY_MODEL_8e7c55cbca624432a84fa7ad8f3a4016"
            ],
            "layout": "IPY_MODEL_9ee03f9c85f34155b2645e89c9211547"
          }
        },
        "933ebc451c09490aadf71afbbb3dff2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5c4f3d1c8b046e3a163faaa6b3a51ab",
            "max": 22090455,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd62d83b35d04a178840772e82bd2f2e",
            "value": 22090455
          }
        },
        "8e7c55cbca624432a84fa7ad8f3a4016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d28208ba1213436a93926a01d99d97ae",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_78d1da8efb504b03878ca9ce5b404006",
            "value": " 21.1M/21.1M [00:01&lt;00:00, 16.9MB/s]"
          }
        },
        "9ee03f9c85f34155b2645e89c9211547": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5c4f3d1c8b046e3a163faaa6b3a51ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd62d83b35d04a178840772e82bd2f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "d28208ba1213436a93926a01d99d97ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78d1da8efb504b03878ca9ce5b404006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kapetis/Video-Analytics/blob/main/Demo_People_Counting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvhYZrIZCEyo"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/26833433/98702494-b71c4e80-237a-11eb-87ed-17fcd6b3f066.jpg\">\n",
        "\n",
        "This notebook was written by Ultralytics LLC, and is freely available for redistribution under the [GPL-3.0 license](https://choosealicense.com/licenses/gpl-3.0/). \n",
        "For more information please visit https://github.com/ultralytics/yolov5 and https://www.ultralytics.com."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Clone repo, install dependencies and check PyTorch and GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvMlHd_QwMG",
        "outputId": "372afe74-0803-46c9-df88-198a9b6c5529"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.7.0+cu101 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15079MB, multi_processor_count=40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4i8Rh90m-Bw",
        "outputId": "fcb54f20-e38b-4d1a-9ed4-86a6c611d0c3"
      },
      "source": [
        "!git clone https://github.com/maxmarkov/track_and_count\n",
        "!pip3 install -r track_and_count/requirements.txt --quie"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'track_and_count'...\n",
            "remote: Enumerating objects: 457, done.\u001b[K\n",
            "remote: Counting objects: 100% (457/457), done.\u001b[K\n",
            "remote: Compressing objects: 100% (331/331), done.\u001b[K\n",
            "remote: Total 457 (delta 181), reused 351 (delta 91), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (457/457), 55.64 MiB | 30.26 MiB/s, done.\n",
            "Resolving deltas: 100% (181/181), done.\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184kB 10.0MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122kB 28.2MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.5MB 46.1MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 952kB 47.6MB/s \n",
            "\u001b[?25h  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyyXRuDOoOMN",
        "outputId": "c138d3e3-23da-45b2-bb96-1261bad8e1b8"
      },
      "source": [
        "cd /content/yolov5/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yolov5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22K_7DTenYB2",
        "outputId": "b6532cbe-d8b4-4a00-d3ac-e0e383efe152"
      },
      "source": [
        "%cd track_and_count/yolov5/\n",
        "!weights/download_weights.sh\n",
        "%cd ..\n",
        "!deep_sort/deep_sort/deep/checkpoint/download_weights.sh"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yolov5/track_and_count/yolov5\n",
            "/content/yolov5/track_and_count\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11RdjGz9oBjk",
        "outputId": "2dbf127e-8b1b-46d5-c5e6-c8502f41fae1"
      },
      "source": [
        "!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.6.0+cu101 in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: torchvision==0.7.0+cu101 in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7.0+cu101) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sP5uXb6nMA-",
        "outputId": "0b15f3ea-e36d-472b-cad1-1376b318426c"
      },
      "source": [
        "!python3 deepsort_features.py --source '4-cameras.mp4' --weights 'yolov5/weights/yolov5s.pt' --classes 0 --device 0"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(agnostic_nms=False, augment=False, classes=[0], conf_thres=0.4, config_deepsort='deep_sort/configs/deep_sort.yaml', crops='inference/image_crops', device='0', features='inference/features', img_size=640, iou_thres=0.5, output='inference/output', save_txt=False, source='4-cameras.mp4', update=False, view_img=False, weights=['yolov5/weights/yolov5s.pt'])\n",
            "Using CUDA device0 _CudaDeviceProperties(name='Tesla T4', total_memory=15079MB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 191 layers, 7.46816e+06 parameters, 0 gradients\n",
            "FRAMES PER SECOND  29.980334443002018\n",
            "Loading weights from deep_sort/deep_sort/deep/checkpoint/ckpt.t7... Done!\n",
            "video 1/1 (1/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.018s)\n",
            "video 1/1 (2/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.014s)\n",
            "video 1/1 (3/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.013s)\n",
            "video 1/1 (4/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (5/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (6/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (7/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (8/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (9/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.012s)\n",
            "video 1/1 (10/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.012s)\n",
            "video 1/1 (11/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.012s)\n",
            "video 1/1 (12/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (13/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (14/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (15/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (16/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.012s)\n",
            "video 1/1 (17/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.014s)\n",
            "video 1/1 (18/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (19/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (20/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.016s)\n",
            "video 1/1 (21/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.012s)\n",
            "video 1/1 (22/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.012s)\n",
            "video 1/1 (23/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (24/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (25/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (26/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (27/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.012s)\n",
            "video 1/1 (28/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.015s)\n",
            "video 1/1 (29/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.012s)\n",
            "video 1/1 (30/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.011s)\n",
            "video 1/1 (31/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.012s)\n",
            "video 1/1 (32/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.012s)\n",
            "video 1/1 (33/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.015s)\n",
            "video 1/1 (34/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.013s)\n",
            "video 1/1 (35/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.015s)\n",
            "video 1/1 (36/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.013s)\n",
            "video 1/1 (37/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.014s)\n",
            "video 1/1 (38/1555) /content/yolov5/track_and_count/4-cameras.mp4: 384x640 Done. (0.012s)\n",
            "video 1/1 (39/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 1 people were detected!\n",
            "384x640 1 persons, Done. (0.016s)\n",
            "video 1/1 (40/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 1 people were detected!\n",
            "384x640 1 persons, Done. (0.014s)\n",
            "video 1/1 (41/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 2 people were detected!\n",
            "384x640 2 persons, Done. (0.013s)\n",
            "video 1/1 (42/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 3 people were detected!\n",
            "384x640 3 persons, Done. (0.013s)\n",
            "video 1/1 (43/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 3 people were detected!\n",
            "384x640 3 persons, Done. (0.013s)\n",
            "video 1/1 (44/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 5 people were detected!\n",
            "384x640 5 persons, Done. (0.013s)\n",
            "video 1/1 (45/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 3 people were detected!\n",
            "384x640 3 persons, Done. (0.013s)\n",
            "video 1/1 (46/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 3 people were detected!\n",
            "384x640 3 persons, Done. (0.017s)\n",
            "video 1/1 (47/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 3 people were detected!\n",
            "384x640 3 persons, Done. (0.013s)\n",
            "video 1/1 (48/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 3 people were detected!\n",
            "384x640 3 persons, Done. (0.013s)\n",
            "video 1/1 (49/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 4 people were detected!\n",
            "384x640 4 persons, Done. (0.013s)\n",
            "video 1/1 (50/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 4 people were detected!\n",
            "384x640 4 persons, Done. (0.013s)\n",
            "video 1/1 (51/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 4 people were detected!\n",
            "384x640 4 persons, Done. (0.013s)\n",
            "video 1/1 (52/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 5 people were detected!\n",
            "384x640 5 persons, Done. (0.013s)\n",
            "video 1/1 (53/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 6 people were detected!\n",
            "384x640 6 persons, Done. (0.013s)\n",
            "video 1/1 (54/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 6 people were detected!\n",
            "384x640 6 persons, Done. (0.013s)\n",
            "video 1/1 (55/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 6 people were detected!\n",
            "384x640 6 persons, Done. (0.012s)\n",
            "video 1/1 (56/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 6 people were detected!\n",
            "384x640 6 persons, Done. (0.012s)\n",
            "video 1/1 (57/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 6 people were detected!\n",
            "384x640 6 persons, Done. (0.016s)\n",
            "video 1/1 (58/1555) /content/yolov5/track_and_count/4-cameras.mp4: \n",
            " 5 people were detected!\n",
            "Traceback (most recent call last):\n",
            "  File \"deepsort_features.py\", line 249, in <module>\n",
            "    detect()\n",
            "  File \"deepsort_features.py\", line 163, in detect\n",
            "    savetxt(filename+'.csv', features[track_id], delimiter=',')\n",
            "  File \"<__array_function__ internals>\", line 6, in savetxt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\", line 1447, in savetxt\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAVn73BXRFOa"
      },
      "source": [
        "!mkdir foto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# 1. Inference\n",
        "\n",
        "`detect.py` runs inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR9ZbuQCH7FX",
        "outputId": "5baea001-c6a2-4d0f-d1fa-677976efc4c3"
      },
      "source": [
        "!python detect.py --weights yolov5s.pt  --conf 0.5 --source data/4-cameras.mp4 --classes 0 --save-conf  --save-txt \n",
        "#Image(filename='runs/detect/exp/zidane.jpg', width=600)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'detect.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qbaa3iEcrcE"
      },
      "source": [
        "Results are saved to `runs/detect`. A full list of available inference sources:\n",
        "<img src=\"https://user-images.githubusercontent.com/26833433/98274798-2b7a7a80-1f94-11eb-91a4-70c73593e26b.jpg\" width=\"900\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eq1SMWl6Sfn"
      },
      "source": [
        "# 2. Test\n",
        "Test a model on [COCO](https://cocodataset.org/#home) val or test-dev dataset to evaluate trained accuracy. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag. Note that `pycocotools` metrics may be 1-2% better than the equivalent repo metrics, as is visible below, due to slight differences in mAP computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyTZYGgRjnMc"
      },
      "source": [
        "## COCO val2017\n",
        "Download [COCO val 2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L14) dataset (1GB - 5000 images), and test model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "02ac0588602847eea00a0205f87bcce2",
            "091fdf499bd44a80af7281d16da4aa93",
            "c79f69c959de4427ba102a87a9f46d80",
            "c472ea49806447a68b5a9221a4ddae85",
            "5a90f72d3a2d46cb9ad915daa3ead8b4",
            "c42ae5af74a0491187827d0a1fc259bb",
            "fead0160658445bf9e966daa4481cad0",
            "2a7ed6611da34662b10e37fd4f4e4438"
          ]
        },
        "id": "WQPtK1QYVaD_",
        "outputId": "780d8f5f-766e-4b99-e370-11f9b884c27a"
      },
      "source": [
        "# Download COCO val2017\n",
        "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017val.zip', 'tmp.zip')\n",
        "!unzip -q tmp.zip -d ../ && rm tmp.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02ac0588602847eea00a0205f87bcce2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=819257867.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X58w8JLpMnjH",
        "outputId": "013935a5-ba81-4810-b723-0cb01cf7bc79"
      },
      "source": [
        "# Run YOLOv5x on COCO val2017\n",
        "!python test.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(augment=False, batch_size=32, conf_thres=0.001, data='./data/coco.yaml', device='', exist_ok=False, img_size=640, iou_thres=0.65, name='exp', project='runs/test', save_conf=False, save_json=True, save_txt=False, single_cls=False, task='val', verbose=False, weights=['yolov5x.pt'])\n",
            "Using torch 1.7.0+cu101 CUDA:0 (Tesla V100-SXM2-16GB, 16130MB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v3.1/yolov5x.pt to yolov5x.pt...\n",
            "100% 170M/170M [00:05<00:00, 32.6MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 484 layers, 88922205 parameters, 0 gradients\n",
            "Scanning labels ../coco/labels/val2017.cache (4952 found, 0 missing, 48 empty, 0 duplicate, for 5000 images): 5000it [00:00, 14785.71it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 157/157 [01:30<00:00,  1.74it/s]\n",
            "                 all       5e+03    3.63e+04       0.409       0.754       0.672       0.484\n",
            "Speed: 5.9/2.1/7.9 ms inference/NMS/total per 640x640 image at batch-size 32\n",
            "\n",
            "Evaluating pycocotools mAP... saving runs/test/exp/yolov5x_predictions.json...\n",
            "loading annotations into memory...\n",
            "Done (t=0.43s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=4.67s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=92.11s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=13.24s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.492\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.676\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.534\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.318\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.541\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.633\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.376\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.617\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.670\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.493\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.723\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.812\n",
            "Results saved to runs/test/exp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc_KbFk0juX2"
      },
      "source": [
        "## COCO test-dev2017\n",
        "Download [COCO test2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L15) dataset (7GB - 40,000 images), to test model accuracy on test-dev set (20,000 images). Results are saved to a `*.json` file which can be submitted to the evaluation server at https://competitions.codalab.org/competitions/20794."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0AJnSeCIHyJ"
      },
      "source": [
        "# Download COCO test-dev2017\n",
        "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels.zip', 'tmp.zip')\n",
        "!unzip -q tmp.zip -d ../ && rm tmp.zip  # unzip labels\n",
        "!f=\"test2017.zip\" && curl http://images.cocodataset.org/zips/$f -o $f && unzip -q $f && rm $f  # 7GB,  41k images\n",
        "%mv ./test2017 ./coco/images && mv ./coco ../  # move images to /coco and move /coco next to /yolov5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29GJXAP_lPrt",
        "outputId": "1ffe2d63-a2c6-4d33-dbd1-264ec3cb449a"
      },
      "source": [
        "# Run YOLOv5s on COCO test-dev2017 using --task test\n",
        "!python test.py --weights yolov5s.pt --data coco.yaml --task test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"test.py\", line 9, in <module>\n",
            "    import yaml\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/yaml/__init__.py\", line 8, in <module>\n",
            "    from .loader import *\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/yaml/loader.py\", line 5, in <module>\n",
            "    from .scanner import *\n",
            "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 764, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 833, in get_data\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUOiNLtMP5aG"
      },
      "source": [
        "# 3. Train\n",
        "\n",
        "Download [COCO128](https://www.kaggle.com/ultralytics/coco128), a small 128-image tutorial dataset, start tensorboard and train YOLOv5s from a pretrained checkpoint for 3 epochs (note actual training is typically much longer, around **300-1000 epochs**, depending on your dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "cf1ab9fde7444d3e874fcd407ba8f0f8",
            "933ebc451c09490aadf71afbbb3dff2a",
            "8e7c55cbca624432a84fa7ad8f3a4016",
            "9ee03f9c85f34155b2645e89c9211547",
            "d5c4f3d1c8b046e3a163faaa6b3a51ab",
            "dd62d83b35d04a178840772e82bd2f2e",
            "d28208ba1213436a93926a01d99d97ae",
            "78d1da8efb504b03878ca9ce5b404006"
          ]
        },
        "id": "Knxi2ncxWffW",
        "outputId": "59f9a94b-21e1-4626-f36a-a8e1b1e5c8f6"
      },
      "source": [
        "# Download COCO128\n",
        "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip', 'tmp.zip')\n",
        "!unzip -q tmp.zip -d ../ && rm tmp.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf1ab9fde7444d3e874fcd407ba8f0f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=22090455.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pOkGLv1dMqh"
      },
      "source": [
        "Train a YOLOv5s model on [COCO128](https://www.kaggle.com/ultralytics/coco128) with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and **COCO, COCO128, and VOC datasets are downloaded automatically** on first use.\n",
        "\n",
        "All training results are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOy5KI2ncnWd"
      },
      "source": [
        "# Tensorboard (optional)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fLAV42oNb7M"
      },
      "source": [
        "# Weights & Biases (optional)\n",
        "%pip install -q wandb  \n",
        "!wandb login  # use 'wandb disabled' or 'wandb enabled' to disable or enable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NcFxRcFdJ_O",
        "outputId": "138f2d1d-364c-405a-cf13-ea91a2aff915"
      },
      "source": [
        "# Train YOLOv5s on COCO128 for 3 epochs\n",
        "!python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --nosave --cache"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using torch 1.7.0+cu101 CUDA:0 (Tesla V100-SXM2-16GB, 16130MB)\n",
            "\n",
            "Namespace(adam=False, batch_size=16, bucket='', cache_images=True, cfg='', data='./data/coco128.yaml', device='', epochs=3, evolve=False, exist_ok=False, global_rank=-1, hyp='data/hyp.scratch.yaml', image_weights=False, img_size=[640, 640], local_rank=-1, log_imgs=16, multi_scale=False, name='exp', noautoanchor=False, nosave=True, notest=False, project='runs/train', rect=False, resume=False, save_dir='runs/train/exp', single_cls=False, sync_bn=False, total_batch_size=16, weights='yolov5s.pt', workers=8, world_size=1)\n",
            "Start Tensorboard with \"tensorboard --logdir runs/train\", view at http://localhost:6006/\n",
            "2020-11-20 11:45:17.042357: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Hyperparameters {'lr0': 0.01, 'lrf': 0.2, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'warmup_bias_lr': 0.1, 'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'mosaic': 1.0, 'mixup': 0.0}\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v3.1/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.5M/14.5M [00:01<00:00, 14.8MB/s]\n",
            "\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     19904  models.common.BottleneckCSP             [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  1    161152  models.common.BottleneckCSP             [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  1    641792  models.common.BottleneckCSP             [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    378624  models.common.BottleneckCSP             [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     95104  models.common.BottleneckCSP             [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model Summary: 283 layers, 7468157 parameters, 7468157 gradients\n",
            "\n",
            "Transferred 370/370 items from yolov5s.pt\n",
            "Optimizer groups: 62 .bias, 70 conv.weight, 59 other\n",
            "Scanning images: 100% 128/128 [00:00<00:00, 5395.63it/s]\n",
            "Scanning labels ../coco128/labels/train2017.cache (126 found, 0 missing, 2 empty, 0 duplicate, for 128 images): 128it [00:00, 13972.28it/s]\n",
            "Caching images (0.1GB): 100% 128/128 [00:00<00:00, 173.55it/s]\n",
            "Scanning labels ../coco128/labels/train2017.cache (126 found, 0 missing, 2 empty, 0 duplicate, for 128 images): 128it [00:00, 8693.98it/s]\n",
            "Caching images (0.1GB): 100% 128/128 [00:00<00:00, 133.30it/s]\n",
            "NumExpr defaulting to 2 threads.\n",
            "\n",
            "Analyzing anchors... anchors/target = 4.26, Best Possible Recall (BPR) = 0.9946\n",
            "Image sizes 640 train, 640 test\n",
            "Using 2 dataloader workers\n",
            "Logging results to runs/train/exp\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "       0/2     5.24G   0.04202   0.06745   0.01503    0.1245       194       640: 100% 8/8 [00:03<00:00,  2.01it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 8/8 [00:03<00:00,  2.40it/s]\n",
            "                 all         128         929       0.404       0.758       0.701        0.45\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "       1/2     5.12G   0.04461   0.05874    0.0169    0.1202       142       640: 100% 8/8 [00:01<00:00,  4.14it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 8/8 [00:01<00:00,  5.75it/s]\n",
            "                 all         128         929       0.403       0.772       0.703       0.453\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "       2/2     5.12G   0.04445   0.06545   0.01667    0.1266       149       640: 100% 8/8 [00:01<00:00,  4.15it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 8/8 [00:06<00:00,  1.18it/s]\n",
            "                 all         128         929       0.395       0.767       0.702       0.452\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 15.2MB\n",
            "3 epochs completed in 0.006 hours.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15glLzbQx5u0"
      },
      "source": [
        "# 4. Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLI1JmHU7B0l"
      },
      "source": [
        "## Weights & Biases Logging ðŸŒŸ NEW\n",
        "\n",
        "[Weights & Biases](https://www.wandb.com/) (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration for teams. To enable W&B `pip install wandb`, and then train normally (you will be guided through setup on first use). \n",
        "\n",
        "During training you will see live updates at [https://wandb.ai/home](https://wandb.ai/home), and you can create and share detailed [Reports](https://wandb.ai/glenn-jocher/yolov5_tutorial/reports/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY) of your results. For more information see the [YOLOv5 Weights & Biases Tutorial](https://github.com/ultralytics/yolov5/issues/1289). \n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/26833433/98184457-bd3da580-1f0a-11eb-8461-95d908a71893.jpg\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WPvRbS5Swl6"
      },
      "source": [
        "## Local Logging\n",
        "\n",
        "All results are logged by default to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc. View train and test jpgs to see mosaics, labels, predictions and augmentation effects. Note a **Mosaic Dataloader** is used for training (shown below), a new concept developed by Ultralytics and first featured in [YOLOv4](https://arxiv.org/abs/2004.10934)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riPdhraOTCO0"
      },
      "source": [
        "Image(filename='runs/train/exp/train_batch0.jpg', width=800)  # train batch 0 mosaics and labels\n",
        "Image(filename='runs/train/exp/test_batch0_labels.jpg', width=800)  # test batch 0 labels\n",
        "Image(filename='runs/train/exp/test_batch0_pred.jpg', width=800)  # test batch 0 predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYG4WFEnTVrI"
      },
      "source": [
        "> <img src=\"https://user-images.githubusercontent.com/26833433/83667642-90fcb200-a583-11ea-8fa3-338bbf7da194.jpeg\" width=\"750\">  \n",
        "`train_batch0.jpg` shows train batch 0 mosaics and labels\n",
        "\n",
        "> <img src=\"https://user-images.githubusercontent.com/26833433/83667626-8c37fe00-a583-11ea-997b-0923fe59b29b.jpeg\" width=\"750\">  \n",
        "`test_batch0_labels.jpg` shows test batch 0 labels\n",
        "\n",
        "> <img src=\"https://user-images.githubusercontent.com/26833433/83667635-90641b80-a583-11ea-8075-606316cebb9c.jpeg\" width=\"750\">  \n",
        "`test_batch0_pred.jpg` shows test batch 0 _predictions_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KN5ghjE6ZWh"
      },
      "source": [
        "Training losses and performance metrics are also logged to [Tensorboard](https://www.tensorflow.org/tensorboard) and a custom `results.txt` logfile which is plotted as `results.png` (below) after training completes. Here we show YOLOv5s trained on COCO128 to 300 epochs, starting from scratch (blue), and from pretrained `--weights yolov5s.pt` (orange)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDznIqPF7nk3"
      },
      "source": [
        "from utils.plots import plot_results \n",
        "plot_results(save_dir='runs/train/exp')  # plot all results*.txt as results.png\n",
        "Image(filename='runs/train/exp/results.png', width=800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfrEegCSW3fK"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/26833433/97808309-8182b180-1c66-11eb-8461-bffe1a79511d.png\" width=\"800\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zelyeqbyt3GD"
      },
      "source": [
        "# Environments\n",
        "\n",
        "YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n",
        "\n",
        "- **Google Colab Notebook** with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "- **Kaggle Notebook** with free GPU: [https://www.kaggle.com/ultralytics/yolov5](https://www.kaggle.com/ultralytics/yolov5)\n",
        "- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart) \n",
        "- **Docker Image** https://hub.docker.com/r/ultralytics/yolov5. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart) ![Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEijrePND_2I"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "Optional extras below. Unit tests validate repo functionality and should be run on any PRs submitted.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI6NoBev8Ib1"
      },
      "source": [
        "# Re-clone repo\n",
        "%cd ..\n",
        "%rm -rf yolov5 && git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcKoSIK2WSzj"
      },
      "source": [
        "# Test all\n",
        "%%shell\n",
        "for x in s m l x; do\n",
        "  python test.py --weights yolov5$x.pt --data coco.yaml --img 640\n",
        "done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGH0ZjkGjejy"
      },
      "source": [
        "# Unit tests\n",
        "%%shell\n",
        "export PYTHONPATH=\"$PWD\"  # to run *.py. files in subdirectories\n",
        "\n",
        "rm -rf runs  # remove runs/\n",
        "for m in yolov5s; do  # models\n",
        "  python train.py --weights $m.pt --epochs 3 --img 320 --device 0  # train pretrained\n",
        "  python train.py --weights '' --cfg $m.yaml --epochs 3 --img 320 --device 0  # train scratch\n",
        "  for d in 0 cpu; do  # devices\n",
        "    python detect.py --weights $m.pt --device $d  # detect official\n",
        "    python detect.py --weights runs/train/exp/weights/best.pt --device $d  # detect custom\n",
        "    python test.py --weights $m.pt --device $d # test official\n",
        "    python test.py --weights runs/train/exp/weights/best.pt --device $d # test custom\n",
        "  done\n",
        "  python hubconf.py  # hub\n",
        "  python models/yolo.py --cfg $m.yaml  # inspect\n",
        "  python models/export.py --weights $m.pt --img 640 --batch 1  # export\n",
        "done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gogI-kwi3Tye"
      },
      "source": [
        "# Profile\n",
        "from utils.torch_utils import profile \n",
        "\n",
        "m1 = lambda x: x * torch.sigmoid(x)\n",
        "m2 = torch.nn.SiLU()\n",
        "profile(x=torch.randn(16, 3, 640, 640), [m1, m2], n=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSgFCAcMbk1R"
      },
      "source": [
        "# VOC\n",
        "for b, m in zip([64, 48, 32, 16], ['yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']):  # zip(batch_size, model)\n",
        "  !python train.py --batch {b} --weights {m}.pt --data voc.yaml --epochs 50 --cache --img 512 --nosave --hyp hyp.finetune.yaml --project VOC --name {m}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}